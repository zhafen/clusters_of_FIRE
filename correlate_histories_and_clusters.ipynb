{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import numba\n",
    "import os\n",
    "import pandas as pd\n",
    "import unyt\n",
    "import verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "import palettable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linefinder.analyze_data.worldlines as worldlines\n",
    "import linefinder.analyze_data.plot_worldlines as plot_worldlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import galaxy_dive.trends.galaxy as gal_trends  \n",
    "import galaxy_dive.plot_data.plotting as plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cosmological History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = worldlines.Worldlines(\n",
    "    data_dir = '/scratch/03057/zhafen/linefinder_data/multiphysics/m12i_res7100_mhdcv/data',\n",
    "    tag = 'm12imhdcv_clustersofFIRE_pop1',\n",
    "    halo_data_dir = '/scratch/03057/zhafen/multiphysics/m12i_res7100_mhdcv/halo',\n",
    "    main_halo_id = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_plotter = plot_worldlines.WorldlinesPlotter( w, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/scratch/03057/zhafen/linefinder_data/multiphysics/m12i_res7100_mhdcv/data'\n",
    "file_name = 'ClusterPopulation_1_reduced.hdf5'\n",
    "file_path = os.path.join( data_dir, file_name )\n",
    "f = h5py.File( file_path, 'r' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "id_data = {}\n",
    "for key in f.keys():\n",
    "    if key == 'Coordinates':\n",
    "        for i in range( 3 ):\n",
    "            id_data['P{}'.format( i )] = f[key][...][:,i]\n",
    "    elif key == 'TracerID':\n",
    "        id_data[key] = f[key][...].astype( int )\n",
    "    else:\n",
    "        id_data[key] = f[key][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.DataFrame(\n",
    "    data = id_data,\n",
    ")\n",
    "id_df = id_df.set_index( 'TracerID' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate the Two Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dataframe to find the relevant indices of the particle array\n",
    "loc_df = pd.DataFrame(\n",
    "    {\n",
    "        'ID': w.get_data( 'ID' ).astype( int ),\n",
    "        'inds': np.arange( w.get_data( 'ID' ).size ).astype( int )\n",
    "    }\n",
    ")\n",
    "loc_df = loc_df.set_index( 'ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ind_array to use for creating the file\n",
    "ind_df = loc_df.reindex( id_df.index )\n",
    "is_nan = np.isnan( ind_df['inds'] )\n",
    "is_not_nan = np.invert( is_nan )\n",
    "ind_arr = ind_df['inds'][is_not_nan].values.astype( int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_worldlines_data_for_output( arr, shape=( id_df.index.size, w.n_snaps ) ):\n",
    "    '''Format worldlines data to account for NaN values.'''\n",
    "    \n",
    "    output_arr = np.full( shape, np.nan )\n",
    "    output_arr[is_not_nan] = arr\n",
    "\n",
    "    return output_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Specific Correlated Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the tracer IDs\n",
    "data_out['TracerID'] = format_worldlines_data_for_output( w.get_data('ID')[ind_arr], shape=( id_df.index.size, ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the IDs are stored in the expected order.\n",
    "other_is_not_nan = np.invert( np.isnan( data_out['TracerID'] ) )\n",
    "np.testing.assert_allclose( data_out['TracerID'][other_is_not_nan], id_df.index[other_is_not_nan] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a bunch of additional correlated data\n",
    "for i, key in enumerate( [ 'R', 'M_enc', 'Txx', 'Tyy', 'Tzz', 'Txy', 'Tzx', 'Tyz', 'Vx', 'Vy', 'Vz', 'mt_gal_id' ] ):\n",
    "    data_out[key] = format_worldlines_data_for_output( w.get_data( key )[ind_arr,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "500\n",
      "400\n",
      "300\n",
      "200\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Get and store velocity dispersion interior to R\n",
    "r = w.get_data( 'R' )\n",
    "sigma = np.full( w.base_data_shape, fill_value=np.nan )\n",
    "r_co = r * w.hubble_param * ( 1. + w.redshift )[np.newaxis,:] \n",
    "\n",
    "for i, snum in enumerate( w.snums ):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( snum )\n",
    "    \n",
    "    sigma[:,i] = w.halo_data.get_profile_data(\n",
    "        'sigv',\n",
    "        snum = snum,\n",
    "        r = r_co[:,i],\n",
    "        mt_halo_id = w.main_halo_id,\n",
    "    )\n",
    "\n",
    "data_out['SigmaV'] = format_worldlines_data_for_output( sigma[ind_arr,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the cluster mass\n",
    "data_out['ClusterMass'] = id_df['Mass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the angular momentum\n",
    "ang_mom = ( w.get_data( 'Lmag' ) * unyt.kpc * unyt.km / unyt.s / w.get_data( 'M' ) ).to( 'km**2/s' ).value\n",
    "data_out['SpecificAngularMomentum'] = format_worldlines_data_for_output( ang_mom[ind_arr,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the radius if the particle was on a circular orbit in an isothermal potential\n",
    "data_out['RcIsoPot'] = format_worldlines_data_for_output( w.get_data( 'rc_iso_pot' )[ind_arr,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the current age of the particle\n",
    "formation_inds = 600 - id_df['Snapshot']\n",
    "age_at_formation =  w.get_data( 'time' )[formation_inds]\n",
    "current_age = np.tile( w.get_data( 'time' ), ( age_at_formation.size, 1 ) ) - age_at_formation[:,np.newaxis] \n",
    "data_out['CurrentAge'] = current_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the simulation redshift\n",
    "data_out['SimulationRedshift'] = np.tile( w.get_data( 'redshift' ), ( id_df.index.size, 1 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Dynamical Friction Times\n",
    "And when the clusters should stop being evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Dynamical Friction Times\n",
    "t_friction = gal_trends.dynamical_friction_time(\n",
    "    ang_mom = data_out['SpecificAngularMomentum'] * unyt.km**2. / unyt.s,\n",
    "    r_c = data_out['RcIsoPot'] * unyt.kpc,\n",
    "    v_c = np.tile( w.v_c.values, ( id_df.index.size, 1 ) ) * unyt.km / unyt.s,\n",
    "    mass = np.tile( data_out['ClusterMass'], ( w.n_snaps, 1 ) ).transpose() * unyt.msun,\n",
    "    sigma = data_out['SigmaV'] * unyt.km / unyt.s,\n",
    "    m_enc = data_out['M_enc'] * unyt.msun,\n",
    ").to( 'Gyr' )\n",
    "data_out['DynFrictionTime'] = t_friction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit('i8[:](i8[:],f8[:],f8[:])')\n",
    "def find_final_inds( final_inds, t_df, t_age ):\n",
    "    \n",
    "    for i in range( t_age.shape[0] ):\n",
    "\n",
    "        final_ind = 0\n",
    "        for j in range( t_age.shape[1] ):\n",
    "\n",
    "            # Keep going until we find the earliest indice\n",
    "            if t_df[i,j] < t_age[i,j]:\n",
    "                final_ind = j\n",
    "\n",
    "        final_inds[i] = final_ind\n",
    "        \n",
    "    return final_inds\n",
    "\n",
    "final_inds = find_final_inds( np.zeros( current_age.shape[0], ).astype( int ), data_out['DynFrictionTime'], current_age )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the snapshots and redshifts at which the cluster should no longer be evolved\n",
    "data_out['EndSnapshot'] = w.get_data( 'snum' )[final_inds]\n",
    "data_out['EndRedshift'] = w.get_data( 'redshift' )[final_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure( figsize=(8,8), facecolor='w' )\n",
    "ax = plt.gca()\n",
    "\n",
    "min_fric_times = np.nanmin( t_friction, axis=1 )\n",
    "ax.hist2d(\n",
    "    data_out['ClusterMass'],\n",
    "    min_fric_times.value,\n",
    "    bins = [ np.logspace( 4., 7., 64 ), np.logspace( -4., 7., 64 ), ],\n",
    "    norm = matplotlib.colors.LogNorm(),\n",
    ")\n",
    "\n",
    "hubble_time = ( 1. / ( w.hubble_param * 100. * unyt.km / unyt.s / unyt.Mpc ) ).to( 'Gyr' )\n",
    "ax.axhline(\n",
    "    hubble_time,\n",
    "    color = palettable.cartocolors.qualitative.Safe_10.mpl_colors[1],\n",
    "    linewidth = 5,\n",
    "    linestyle = '--',\n",
    ")\n",
    "\n",
    "ax.set_xlabel( 'Cluster Mass ($M_\\odot$)', fontsize=22 )\n",
    "ax.set_ylabel( 'Minimum Dynamical Friction Time (Gyr)', fontsize=22 )\n",
    "ax.set_xscale( 'log' )\n",
    "ax.set_yscale( 'log' )\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_spiral = ( final_inds != 0 ).sum() / final_inds.size\n",
    "print( '{:.2g}% of the clusters reach t_DF < t_age'.format( frac_spiral*100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Valid Clusters to Evolve\n",
    "This mask selects the subset of clusters that are okay to evolved.\n",
    "Conditions:\n",
    "1. Must be in the main halo at z=0 (~90% of clusters fit this criteria)\n",
    "2. Must not be traced by a particle that splits (~99.95% of clusters fit this criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_halo_id = 0\n",
    "in_target_halo_at_z0 = data_out['mt_gal_id'][:,0].astype( int ) == target_halo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cluster_to_evolve = in_target_halo_at_z0 & is_not_nan\n",
    "data_out['ValidCluster'] = valid_cluster_to_evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish up and Store the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all output data has the expected shape\n",
    "for key, item in data_out.items():\n",
    "    assert item.shape[0] == id_df.index.size, print( key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Data\n",
    "data_out = verdict.Dict( data_out )\n",
    "save_file = 'cosmo_histories_{}.hdf5'.format(w.tag )\n",
    "save_filepath = os.path.join( w.data_dir, save_file )\n",
    "data_out.to_hdf5( save_filepath, )\n",
    "\n",
    "print( 'Data saved at {}'.format( save_filepath ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
